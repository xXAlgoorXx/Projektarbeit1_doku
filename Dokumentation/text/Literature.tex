\chapter{Literature Review}

This literature review explores relevant papers in the field of scene understanding and those addressing the reduction of neural networks in terms of parameters and training time.  
Current state-of-the-art techniques rely on deep learning models for scene understanding tasks.  
The primary objective of scene understanding is to extract semantic information from a given scene.  
This serves as the foundation for various applications such as surveillance, autonomous driving, road safety, vision-guided mobile navigation, and more.  
The significance of this field has grown alongside the rapid advancements in neural networks in recent years.  

A significant portion of the literature focuses on applications in autonomous driving \cite{sceneunderstandingautdriving1}.  
In early research, \acrfull{cnn} models, such as SegNet \cite{SegNet}, were predominantly used for scene understanding tasks.  
However, with the introduction of transformers \cite{attentionisallyouneed}, many researchers transitioned to this newer architecture.  
Subsequently, multimodal networks such as CLIP \cite{clip} and ALIGN \cite{ALIGN} emerged.  
These networks utilize a text and image encoder to learn and predict relationships between image-text pairs.  
Their pre-training enables users to fine-tune them for specific tasks without the need to train an entirely new network.  
For instance, fine-tuning can be achieved by adding a linear layer or employing more sophisticated methods like feature distillation \cite{finetuneclip}.  
Other strategies, such as linear probing \cite{linearprobeclip} and the CLIP adapter \cite{clipadapter}, have demonstrated improvements in task-specific performance.  

Segmentation of scenes can also enhance the ability to observe and analyze specific objects within an image.  
Foundational models like \acrfull{sam} \cite{sam} can be employed for image pre-processing or feature extraction, aiding in these tasks.  

To address the challenges of deploying large networks in resource-constrained environments, techniques such as knowledge distillation have been developed.  
These methods significantly reduce network size while maintaining functionality.  
For example, TinyCLIP \cite{tinyclip} reduces the original network size by 75\%, enabling its use on limited platforms and facilitating edge computing.  
