\chapter{Results}
In this chapter the results and their collection is discussed and interpreted.

\section{Evaluation}

To measure the performance of the Hailo hardware the models were tested in 3 ways:
\begin{itemize}
    \item CPU PC
    \item CPU Raspberry Pi
    \item Hailo hardware accelerator
\end{itemize}
The implementations were made as similiar as possible to ensuer the values are correct and representiv. 
As values to measure the following found usefull:
\begin{itemize}
    \item Throughput(speed)
    \item Accuracy
    \item Mermory(parameter count and size) 
\end{itemize}

For throughput 2 measurments are taken.
The first measures the time from input to output.
This includes calculating the text embeddings, the image embeddings and the dot product of the embeddings.
This measurment is called throughput and is measured in iterations per second (it/s).
The second measures the time it takes to calculate one image embedding.
This measurment is called throughput image and is also measured in iterations per second (it/s).

The accuracy of a model describes how a model performs in labeling pictures as in- or outdoor.
This measurment is taken because it is an easy way to see if a network is working correct but it can easly give a wrong image of the situation because it doesnt take unbalanced datasets in conciredation.
To get a precise answer if the network is working correct the confusion matix or a class report of a model has to evaluated.
Both of these are found in the appendix.

The models get executed on the edge.
Although a Raspberry Pi has plenty of mermory it is always a good practice to include the size of a model as measurment.
Size is measured in memory or parameter count.

\subsection{Without hardware accelerator}
In \cref{result:tab:perfPC} the performance on a PC CPU is displayed.
These values 
These values are used to compare the performance with the hailo hardware accelerator.

\begin{table}[]
    \centering
    \begin{tabular}{l|rrrrr}
    \hline
        Modelname & Accuracy & \makecell{Throughput\\(it/s)} & \makecell{Throughput \\ Image (it/s)} & Throughput std & \makecell{Throughput\\Image std} \\ \hline
        RN50 & 0.982 & 12.46 & 32.48 & 5.33 & 3.18 \\ 
        RN101 & 0.984 & 9.38 & 20.01 & 3.43 & 1.81 \\ 
        RN50x4 & 0.989 & 5.79 & 12.13 & 2.04 & 0.86 \\ 
        RN50x16 & 0.963 & 2.51 & 3.81 & 0.55 & 0.17 \\ 
        RN50x64 & 0.988 & 0.88 & 1.13 & 0.11 & 0.03 \\ 
        TinyCLIP-19M & 0.985 & 20.38 & 54.06 & 8.37 & 3.65 \\ 
        TinyCLIP-30M & 0.988 & 13.59 & 37.53 & 5.72 & 3.31 \\ 
    \end{tabular}
    \caption{Performance table from evalutaion on PC CPU.}
    \label{result:tab:perfPC}
\end{table}

In \cref{result:tab:perfRaspi} the performance measurments from executing on the Raspberry Pi CPU are displayed.
RN50x16 and RN50x64 are missing because they are too big to evaluate.
As one can expect the throughput on the Raspberry Pi is significant lower than the throughput on the PC CPU from \cref{result:tab:perfPC} because of the lower computing power.
Intresting is that the accurcy from \cref{result:tab:perfRaspi} is slightly different to \cref{result:tab:perfPC}.
This can be explaind by diffrent CPU architectures and some floating point rounding.
In {result:tab:perfRaspi} the parameter count of the image encoder is displayed. These values are the same for the models on PC and Raspberry Pi.
\begin{table}[]
    \centering
    \begin{tabular}{l|rrrrr}
    \hline
    Modelname & Accuracy & \makecell{Throughput\\(it/s)} & \makecell{Throughput \\ Image (it/s)} & Throughput std & \makecell{Throughput\\Image std} \\ \hline
        RN50 & 0.979 & 1.15 & 2.85 & 0.36 & 0.05 \\ 
        RN101 & 0.982 & 0.92 & 1.89 & 0.24 & 0.038 \\ 
        RN50x4 & 0.988 & 0.61 & 1.13 & 0.14 & 0.0162 \\ 
        TinyCLIP-19M & 0.982 & 2.27 & 5.22 & 0.63 & 0.16 \\ 
        TinyCLIP-30M & 0.983 & 1.60 & 3.79 & 0.47 & 0.09 \\ 
    \end{tabular}
    \caption{Performance table from evalutaion on Raspberry Pi CPU.}
    \label{result:tab:perfRaspi}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{l|rr}
    \hline
    Model name & \begin{tabular}[c]{@{}l@{}}Parameters\\ Visual(Mio)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Parameters\\ Text(Mio)\end{tabular} \\ \hline
    RN50                & 38.3  & 37.8  \\
    RN50x4              & 87.1  & 59.1  \\
    RN50x16             & 167.3 & 850.5 \\
    RN50x64             & 420.4 & 151.1 \\
    RN101               & 56.3  & 37.8  \\
    TinyCLIP-19M & 18.5  & 18.9  \\
    TinyCLIP-30M & 29.5  & 28.3  \\ 
    \end{tabular}
    \caption{Paramter count for different CLIP implementations with ResNet as visual encoder}
    \label{result:tab:clipsize}
\end{table}



\subsection{With hardware accelerator}
\begin{table}[h]
    \centering
    \begin{tabular}{l|rr}
    \hline
    Model name & \begin{tabular}[c]{@{}l@{}}Parameters\\ Visual(Mio)\end{tabular} \\ \hline
    RN50                & 36.2  \\
    RN50x4              & 85.3  \\
    RN50x16             & 164.6 \\
    RN101               & 55.1 \\
    TinyCLIP-19M & 17.1  \\
    TinyCLIP-30M & 27.7  \\ 
    \end{tabular}
    \caption{Paramter count for different CLIP implementations with ResNet as visual encoder}
    \label{result:tab:clipsizequant}
\end{table}
Now networks after compilation are examined.
First we take a look on the paramter count.
There is a slight decrease in paramters but this can be explaind.
In the case of TinyCLIP-19M the cut off graph consists of a matrix multipication with a matrix of the shape \(1024 \times 1408\).
With 
\begin{equation*}
    1024 \times 1408 + 1024 = 1'442'816
\end{equation*}
we get the parameter count for the cut off graph.
Now this gets added to the paramter count of the model and this results in 
\begin{equation*}
    1.4 \text{M} + 17.1 \text{M} = 18.5 \text{M} 
\end{equation*}
which is the parameter count before compilation(see \cref{result:tab:clipsize}).
This calculation can also be done for all other models but the result is the same.
This result means that the architecture isnt affected by the \acrshort{dfc}.
It only changes the quantisation.
Because of quantisation the mermory size should be around 4 times lower than before because the datatype which changes from FP32 to INT8.
This assumtion cant be verfied because the datatype from before and after arent the same.




\section{Conclusion}

\subsection{Execute models on the edge}

\subsection{Working with Hailo}

\section{Outlook}
test