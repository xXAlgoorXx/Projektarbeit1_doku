\chapter{Results}  
In this chapter, the results are presented, discussed, and interpreted.  
The conclusions are drawn based on the findings from the previous chapters of this work.  

\section{Execution of Models on the Edge}  
The successful compilation of a neural network requires an understanding of which parts of the network can be quantized without significant loss of accuracy.
The quantization process is completely handled by the \acrshort{dfc}.
As no \acrshort{gpu} was available, all models were quantized to 8 bits. 
Two implementations were evaluated in this project.  
The first aimed to perform as many computations as possible on the edge, while the second considered the quantization quality of individual layers.  
This was based on the hypothesis raised by a advisor that large matrix multiplications may not quantize effectively (see \cref{methods:sec:cutlocation}).
  

Unfortunately, both implementations yielded unsatisfactory results.  
Consequently, no definitive statements could be made regarding the impact of layer quantization quality.  

All tested models experienced a reduction in accuracy after quantization.  
This effect was particularly severe for models with low parameter counts, such as RN50 and TinyCLIP-19M.
The reason for this could be that the low-parameter networks have little or no redundant weights.
The redundancy could, for example, help to reduce the quantisation error so that the sum of the quantisation errors of all the redundant weights is close to zero.
Interestingly, despite having fewer parameters than RN50, TinyCLIP-30M retained relatively high accuracy after quantization.    

\section{Evaluation of Hailo}  
The Hailo 8L hardware accelerator was found to be an effective product.  
Its combination with the Raspberry Pi 5 facilitated the development of edge AI solutions, offering a simpler alternative to creating custom PCBs.  

The \acrshort{dfc} contributed to this simplicity by providing built-in tutorials and comprehensive guides.  
The guides included code examples for compiling networks with the \acrshort{dfc}, although it should be noted that full functionality of the \acrshort{dfc} requires a GPU. 
It is suspected that the full potential of the \acrshort{dfc} was not utilized in this project due to the absence of a GPU.  

Most of Hailo's examples found in the model zoo\cite{hailo_model_zoo}, Hailos Raspberry Pi examples\cite{hailo_rpi5_examples} and some more generic examples\cite{hailo_application_code_examples} focuse on image processing.

Hailo has no offical support.
They are handeling questions and requests through their Hailo community site.  
While this was helpful for addressing common issues, complex problems, such as those involving dimension swapping (see \cref{implementation:sec:translation}), revealed gaps in the support team's expertise.
Questions posted in the Hailo community site received responses within 1â€“10 days. 
It became apparent that certain network compilations were not feasible with the current tools.  

The Python \acrfull{api} was accessible online and in the \acrshort{dfc} guide but was not well-documented.  
While the code provided by Hailo is open source and supplemented with examples.
These examples also lacked detailed descriptions, requiring developers to independently interpret the functionality.  
 

\section{Conclusion}
The implementation on the PC was done very fast due to the reuse of Lia Winkler's code.
All CLIP implementations are easy to use due to their well-designed \acrshort{api}.
The TinyCLIP \acrshort{api} is designed to be as close as possible to the original CLIP implementation.
This made the adaptation from CLIP to TinyCLIP very easy.
The implementation on the Raspberry Pi proved to be more diffcult.
The constraints from the \acrshort{dfc} made the quantization pretty laborious.
Using the Python \acrshort{api} was also difficult due to the lack of documentation.

This project demonstrated that quantizing models with low parameter counts results in significant accuracy degradation.
This was evident in the evaluation of RN50 and TinyCLIP-19M.  
Conversely, TinyCLIP-30M retained a larger portion of its accuracy after quantization, indicating that the model's inherent capacity may play a role in mitigating quantization losses.  
It must be emphasized that none of the models achieved satisfactory accuracy after quantization.  
The measures implemented to improve accuracy were mostly ineffective.
Only the changes to the threshold achieved some improvements. 
Future efforts should focus on optimizing the models prior to quantization to enhance post-quantization performance.  
Additionally, appropriate hardware, such as GPUs, should be utilized to reduce compilation times and enable advanced \acrshort{dfc} functionalities.  

To fully leverage the Hailo hardware accelerator and the CLIP model, it is recommended to await the implementation of transformer support.
CLIP models which use transformers as vision encoders have higher accuracy than the ones which use ResNets. 
This may also address the limitation of floating-point calculations for text embeddings.  
Quantizing text embeddings alongside the models could lead to improved accuracy.  
Furthermore, the ability to quantize transformers effectively and achieve high performance remains to be established.   

\section{Outlook}
Future work should consider waiting for updates that enable transformer support in the \acrshort{dfc}.  
It is important to validate the hypothesis that matrix multiplications are challenging to quantize effectively, as these operations form the core of self-attention layers in transformers (see \cref{equ:selfattention}).  

Of the two TinyCLIP models tested, only one (TinyClip-30M) had a reasonably usable accuracy after quantization.
This is surprising as it uses fewer parameters than the smallest ResNet tested, but still outperformed it.
This suggests that the process used to create TinyCLIP works well after quantization if the original model has a sufficiently large capacity.
To confirm this suspicion, the Transformer solutions from TinyCLIP should also be evaluated once it is possible to use them on Hailo.

Once the \acrshort{dfc} supports full-model compilation, additional measures, such as fine-tuning and utilizing CLIP adapter \cite{clipadapter}, can be implemented to improve accuracy before quantization.  
For optimal performance, both the text and image encoders should be quantized together to avoid costly context switches during processing.  

To further enhance scene understanding capabilities, preprocessing with SAM \cite{sam} could be employed.  
SAM segments images into distinct regions, which can then be classified by CLIP.  
This approach would likely require two hardware accelerators, one for each model, making the Raspberry Pi 5 unsuitable due to its single M.2 slot.  
To reduce dependency on Hailo, alternative solutions, such as the ME1076 and MM1076 from Mythic, should be evaluated as they become available.

\subsubsection*{Next Steps}
The Next steps should look like this:
\begin{enumerate}
    \item First an evaluation should be made if Transformers really achieve better results than ResNets on the Hexagon dataset.
    The original CLIP paper \cite{clip} states that the Transformer models in general are better than the ResNet models.
    \item If the update from Hailo comes to enable the compilation of Transformers, the full power of the \acrshort{dfc} should be explored.
    This includes different quantisations and optimizations.
    As stated in this document, a GPU is needed to use the full potential of the \acrshort{dfc}.
    \item If Hailo's updates to the \acrshort{dfc} to support Transformers take too long, other options that follow a similar procedure should be explored, such as Mythic's ME1076 and MM1076, or other hardware accelerators mentioned in the market analysis \cref{chapter:marketanalysis}.
    \item As an alternative to the previous point, hardware accelerator chips could be explored.
    For example, ST Microelectronics and ARM offer hardware accelerator chips that can be integrated into custom designs.
\end{enumerate}

