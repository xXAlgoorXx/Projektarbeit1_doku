\chapter{Implementation
    \label{chapter:implementation}}

This chapter describes the tools, methods, and rationale behind the implementation. 
As mentioned earlier, based on initial tests, CLIP and TinyCLIP were selected as the models for this work.


\section{Model Acquisition}

The CLIP models were sourced from the official CLIP GitHub repository \cite{clipgit}.
Similarly, the TinyCLIP models were obtained from the official TinyCLIP GitHub repository \cite{tinyclipgit}.
Alternatively, models can be retrieved from HuggingFace \cite{huggingface}.
However, the challenge with HuggingFace is that the models cannot be split into separate image and text encoders.
For this reason, only models from the official CLIP and TinyCLIP repositories were used.
In \cref{methods:tab:clipsize}, the parameter count for all CLIP and TinyCLIP implementations using a ResNet as the visual encoder is provided.
It is evident that TinyCLIP uses significantly fewer parameters than the original CLIP.

\section{Model Translation
\label{implementation:sec:translation}}

The models were initially split into separate image and text encoders.
The image encoder was then converted into an ONNX graph, which was subsequently simplified.
This ONNX graph was later translated by the \acrshort{dfc} into \acrshort{har} and \acrshort{hef} files.


\begin{figure}
    \centering
    \subfloat[End of CLIP ResNet50x4 vision encoder ONNX graph acquired through official CLIP]{\includegraphics[width=0.4\textwidth]{Images/Implementation/ClipRes50x4.png}\label{fig:implementation:clipres50x4}}
    \qquad
    \subfloat[End of CLIP ResNet50x4 vision encoder ONNX graph sent by Hailo]{\includegraphics[width=0.4\textwidth]{Images/Implementation/HailoClipRes50x4.png}\label{fig:implementation:hailoclipres50x4}}
    \caption{Comparison of CLIP ResNet50x4 vision encoder ONNX graph (a) from official CLIP and (b) from Hailo. The key difference is in the number of dimensions.}
    \label{fig:implementation:compareRN50x4}
\end{figure}

\subsection{Splitting the Graph}

The first major issue encountered during the translation step is that the \acrshort{dfc} cannot compile transformers.
Specifically, the problem arises from a transpose block at the end of every self-attention layer.
The \acrshort{dfc} swaps dimensions and combines the last two dimensions, leading to conflicts.
In the ResNet model obtained through the official CLIP repository (see \cref{fig:implementation:clipres50x4}), the transpose block after the MatMul block swaps the second and third dimensions.
This results in a compilation error because the \acrshort{dfc} has already combined the last two dimensions.

This behavior was discussed with Hailo, and they provided an example where CLIP is run on their hardware. Upon testing, we observed that the error does not occur when using the model provided by Hailo.
The reason for this discrepancy is that the networks used by Hailo were acquired with an older version of CLIP and the ONNX exporter.
While the graphs are functionally equivalent, the architecture differs slightly.
As seen in \cref{fig:implementation:hailoclipres50x4}, the model provided by Hailo has a maximum of three dimensions.
To work around this problem, the ONNX graph is cut off between the last matmul and transpose block.
For this task, the Onnx-Modifier tool \cite{onnxmodifier} is used.
The removed graph are implemented as postprocessing steps.
%As described in \cref{section:dfc}, a script can be applied to adjust the behavior of the \acrshort{dfc}.
After quantization, the graph can again be visualized using a tool called Profiler from Hailo.
In \cref{fig:implementation:compareRN50x4qunathar}, the compiled model with the cutoff end is visualized.
In this figure, the combination of the last dimensions can be seen.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{Images/Implementation/ClipRes50x4_qunat_Har.png}
    \caption{Output of the \acrshort{dfc} after compiling the modified ONNX graph of the ResNet50x4.}
    \label{fig:implementation:compareRN50x4qunathar}
\end{figure}

This graph is then compiled into a \acrshort{hef} file, which can be executed on the Hailo hardware.
To use CLIP, the text embeddings are also required.
Due to the limitation that Transformers cannot be used with Hailo, the embeddings are calculated on a PC and saved in a JSON file.

\subsection{Rest of the Graph}

As mentioned earlier, the cut-off part of the ONNX graph is handled during post-processing. 
The cut-off part consists of one matrix multiplication and some rearranging of dimensions. 
In the first implementation, the weights for the matrix multiplication were extracted and saved in a JSON file.
In a second attempt, the cut-off ONNX graph was directly used.
The second attempt proved to be faster and used less memory.

\section{Inference}

To ensure a fair comparison of performance, the implementation on the Raspberry Pi closely mirrors the one used on the PC. 
The PC application follows a similar structure to the one used by Lia Winkler in her work.
Due to the limitations of the \acrshort{dfc}, only ResNets are used as the vision encoder.

To utilize the vision model on Hailo, the Python API is employed. 
Two different implementations were tested.

The first implementation is based on a code snippet found in \cite{hailoimplementation}.
In this implementation, the device is initialized every time an output is calculated, which significantly slows down the program and results in no substantial speedup.

The second implementation addresses this issue by initializing the device once at the start of the program. 
% With this approach, a speedup factor of 2 is achieved when compared to the evaluation on the PC CPU.
% However, the accuracy significantly decreases after quantization.

\subsection{Text Input}

For the text input, the subclasses from \cref{tab:dataset:subclasses} are used. 
The text inputs are incorporated into sentences.
The text embeddings consist of the subclasses from \cref{tab:dataset:subclasses}, wrapped in sentences.
In \cite{clip}, the authors state that the performance of the network can improve if the text inputs are framed within sentences.
The sentences look like this:
\begin{itemize}
    \item A photo of a \dots
    \item A picture of a \dots
    \item An image of a \dots
    \item A \dots scene
    \item A picture showing a scene of a \dots
\end{itemize}

\subsection{Image Input}
The Hexagon dataset is used as image input.
It consists of panorama images. 
These images are divided into 5 equal-sized patches, and each patch is processed with CLIP.
To process one of these patches with CLIP, it must first undergo pre-processing by resizing it to the required input size.
The pre-processing steps consist of the following:

\begin{itemize}
    \item Resize
    \item Center-crop
    \item If not already in RGB, convert the image to RGB
    \item Normalize
\end{itemize}

Normalization values are calculated over the entire Hexagon dataset.
Finally, a majority vote is taken over the subclasses of the patches to classify an image. 
This process, mentioned in Lia Winkler's report, helps improve the accuracy of the predictions.

\section{Evaluation}

To measure the performance of the Hailo hardware, the models were tested in three different environments:
\begin{itemize}
    \item CPU on PC
    \item CPU on Raspberry Pi
    \item Hailo hardware accelerator
\end{itemize}
The implementations were made as similar as possible to ensure the values are correct and representative. 

The following metrics were used to measure performance:
\begin{itemize}
    \item Throughput (speed)
    \item Accuracy
    \item Size (parameter count and Memory)
\end{itemize}

For throughput, two measurements were taken:
The first measures the time from input to output, including calculating the text embeddings, the image embeddings, and the dot product of the embeddings. This measurement is called throughput and is measured in iterations per second (it/s).
The second measures the time it takes to calculate one image embedding. This measurement is called throughput image and is also measured in iterations per second (it/s).

Accuracy describes how well a model performs in labeling images as either indoor or outdoor. This metric is useful for checking if the network works correctly, but it may be misleading because it doesn't account for unbalanced datasets. To get a more precise understanding of how well the model performs, a confusion matrix or a classification report must be evaluated. Both of these are provided in the appendix. In the text, accuracy is also used to refer to the models ability to classify images correctly.

Although the Raspberry Pi has ample memory, it is always good practice to consider the size of a model as a metric. Size is measured in terms of memory or parameter count.

\subsection{Without Hardware Accelerator}
In \cref{methods:tab:perfPC}, the performance on a PC CPU is displayed. These values are used to compare the performance with the Hailo hardware accelerator.

\begin{table}[tbp]
    \centering
    \begin{tabular}{l|rrrrr}
    \hline
        Model Name & Accuracy & \makecell{Throughput\\(it/s)} & \makecell{Throughput \\ Image (it/s)} & Throughput std & \makecell{Throughput\\Image std} \\ \hline
        RN50 & 0.982 & 12.46 & 32.48 & 5.33 & 3.18 \\ 
        RN50x4 & 0.989 & 5.79 & 12.13 & 2.04 & 0.86 \\ 
        RN50x16 & 0.963 & 2.51 & 3.81 & 0.55 & 0.17 \\ 
        RN50x64 & 0.988 & 0.88 & 1.13 & 0.11 & 0.03 \\
        RN101 & 0.984 & 9.38 & 20.01 & 3.43 & 1.81 \\  
        TinyCLIP-19M & 0.985 & 20.38 & 54.06 & 8.37 & 3.65 \\ 
        TinyCLIP-30M & 0.988 & 13.59 & 37.53 & 5.72 & 3.31 \\ 
    \end{tabular}
    \caption{Performance table from evaluation on PC CPU.}
    \label{methods:tab:perfPC}
\end{table}

In \cref{methods:tab:perfRaspi}, the performance measurements from executing on the Raspberry Pi CPU are displayed. RN50x16 and RN50x64 are missing because they are too large to evaluate on the Raspberry Pi.

As expected, the throughput on the Raspberry Pi is significantly lower than the throughput on the PC CPU from \cref{methods:tab:perfPC}, due to the lower computing power of the Raspberry Pi. Interestingly, the accuracy in \cref{methods:tab:perfRaspi} differs slightly from \cref{methods:tab:perfPC}. This can be explained by the differences in CPU architectures and some floating point rounding.

In \cref{methods:tab:perfRaspi}, the parameter count of the image encoder is displayed. These values are the same for the models on both the PC and the Raspberry Pi.


\begin{table}[]
    \centering
    \begin{tabular}{l|rrrrr}
    \hline
    Modelname & Accuracy & \makecell{Throughput\\(it/s)} & \makecell{Throughput \\ Image (it/s)} & Throughput std & \makecell{Throughput\\Image std} \\ \hline
        RN50 & 0.979 & 1.15 & 2.85 & 0.36 & 0.05 \\ 
        RN50x4 & 0.988 & 0.61 & 1.13 & 0.14 & 0.0162 \\
        RN101 & 0.982 & 0.92 & 1.89 & 0.24 & 0.038 \\  
        TinyCLIP-19M & 0.982 & 2.27 & 5.22 & 0.63 & 0.16 \\ 
        TinyCLIP-30M & 0.983 & 1.60 & 3.79 & 0.47 & 0.09 \\ 
    \end{tabular}
    \caption{Performance evaluation on Raspberry Pi CPU.}
    \label{methods:tab:perfRaspi}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{l|rr}
    \hline
    Model name & \begin{tabular}[c]{@{}l@{}}Parameters\\ Visual (Mio)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Parameters\\ Text (Mio)\end{tabular} \\ \hline
    RN50                & 38.3  & 37.8  \\
    RN50x4              & 87.1  & 59.1  \\
    RN50x16             & 167.3 & 850.5 \\
    RN50x64             & 420.4 & 151.1 \\
    RN101               & 56.3  & 37.8  \\
    TinyCLIP-19M        & 18.5  & 18.9  \\
    TinyCLIP-30M        & 29.5  & 28.3  \\ 
    \end{tabular}
    \caption{Parameter count for different CLIP implementations with ResNet as the visual encoder.}
    \label{methods:tab:clipsize}
\end{table}

\subsection{With Hardware Accelerator}

Now, the networks after compilation are examined. First, we take a look at the parameter count.
There is a slight decrease in parameters, but this can be explained.

In the case of TinyCLIP-19M, the cut-off graph consists of a matrix multiplication with a matrix of the shape \(1024 \times 1408\). With the following calculation:

\begin{equation*}
    1024 \times 1408 + 1024 = 1'442'816
\end{equation*}

we get the parameter count for the cut-off graph. Now, this is added to the parameter count of the model, resulting in:

\begin{equation*}
    1.4 \text{M} + 17.1 \text{M} = 18.5 \text{M} 
\end{equation*}

which is the parameter count before compilation (see \cref{methods:tab:clipsize}).
This calculation can also be done for all other models, but the result is the same.
This result means that the architecture isn't affected by the \acrshort{dfc}; it only changes quantization.
Because of quantization, the memory size should be around four times lower than before, as the datatype of the weights changes from FP32 to INT8.
In reality, the factor is around 3.
This can be seen in \cref{methods:tab:sizecompare}.
It is important to mention that these numbers should be taken with a grain of salt because the datatypes of the saved models are different.


\subsection{Model Size Comparison}

In \cref{methods:tab:sizecompare}, a comparison between the file sizes of ONNX and HEF models after compilation for various CLIP configurations is presented.
As expected, the ONNX models are significantly larger due to the higher precision (FP32), while the HEF files, which are quantized to INT8, are smaller.
This size reduction is a direct result of the \acrshort{dfc}'s quantization process.
For example, the model "RN50" has a file size of 153.2 MB in ONNX format, but after being compiled to HEF, it reduces to 53.7 MB.
Similar reductions in size can be observed for other models such as RN50x4, RN101, TinyClip19M, and TinyClip30M.
This reduction improves memory efficiency when deploying these models on hardware accelerators like Hailo.

\begin{table}
    \centering
    \begin{tabular}{l|rr}
    \hline
    Modelname & Onnx & \acrshort{hef}\\\hline
    RN50 & 153.2 MB & 53.7 MB \\ 
    RN50x4 & 348.4 MB & 121.7 MB  \\ 
    RN101 & 224.9 MB  & 82.7 MB \\
    TinyClip19M & 74.3 MB & 28.6 MB  \\ 
    TinyClip30M & 118.3 MB & 43.5 MB  \\ 
    \end{tabular}
    \caption{Comparison in size between ONNX and HEF file of a CLIP model.}
    \label{methods:tab:sizecompare}
\end{table}

\begin{table}[!h]
    \centering
    \begin{tabular}{l|rrrrr}
    \hline
    Modelname & Accuracy & \makecell{Throughput\\(it/s)} & \makecell{Throughput \\ Image (it/s)} & Throughput std & \makecell{Throughput\\Image std} \\ \hline
    RN50    & 0.983 & 20.63 & 24.58 & 1.341 & 0.27 \\ 
    RN50x4  & 0.982 & 10.13 & 10.75 & 0.32 & 0.059 \\ 
    RN101   & 0.981 & 15.61 & 17.09 & 0.49 & 0.081 \\ 
    TinyClip19M & 0.017 & 30.41 & 39.29 & 2.71 & 3.02 \\ 
    TinyClip30M & 0.530 & 22.96 & 27.82 & 1.57 & 1.71 \\ 
    \end{tabular}
    \caption{Performance table from evaluation on Hailo 8L.}
    \label{methods:tab:perfHailo}
\end{table}

\subsection{Speed and Accuracy Analysis}

Now, let's examine the performance in terms of speed and accuracy.
In \cref{methods:tab:perfHailo}, we present the performance metrics for models evaluated on the Hailo 8L hardware accelerator.
Notably, the throughput on the Hailo device is much higher compared to the Raspberry Pi CPU (see \cref{methods:tab:perfRaspi}).

On first glance, it appears that accuracy only drops for the TinyCLIP models.
However, a closer look reveals that the accuracy of the ResNet models may also be misleading.
To understand the true performance, we need to examine the confusion matrix for the RN50 model (see \cref{methods:fig:compareConfM}).
Although the reported accuracy appears satisfactory, many images are misclassified, highlighting the importance of looking beyond just the accuracy metric.

\begin{figure}[!h]
    \centering
    \subfloat[][Evaluation on PC]{\includegraphics[width=0.49\textwidth]{Images/appendix/resultsPC/Confusion Matrix 5S(RN50).png}\label{methods:fig:confrn50pc}}
    \subfloat[][Evaluation on Hailo]{\includegraphics[width=0.49\textwidth]{Images/Implementation/Confusion Matrix 5S(RN50)Hailo.png}\label{methods:fig:confrn50hailo}}
    \label{methods:fig:compareConfM}
    \caption{Comparison of confusion matrix from Hailo to PC.}
\end{figure}

There are several possible reasons for the discrepancies between the PC and Hailo evaluations, which are reflected in the confusion matrices:
\begin{itemize}
    \item \textbf{Bad Quantization:} The model's performance could be degraded due to suboptimal quantization, which may result in incorrect classification of the images.
    \item \textbf{Implementation Errors:} There could be a mistake in the implementation, such as errors in the creation of the confusion matrix or during post-processing.
    \item \textbf{Differences in Text Embeddings:} The text embeddings used by the models could differ between platforms, affecting the classification accuracy.
\end{itemize}

\section{Control Implementation
    \label{scetion:methods:contimp}}

The first step in identifying the issue is verifying the creation of the confusion matrix.
Since the same code is used on both Hailo and PC, we can check the consistency of the outputs.
The model outputs are saved to a CSV file, and after comparison, it was confirmed that the confusion matrix generated on Hailo matches the one computed on the PC.

Next, we checked the text embeddings.
As mentioned earlier, the text embeddings are saved in a JSON file.
Possible errors could arise from incorrect saving or rounding of the embeddings during the process.
To verify this, we compared the output of the text encoder with the corresponding JSON file.
In all cases, the differences were found to be zero, indicating that the embeddings were identical across both platforms.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/Implementation/compareProbs_RN50.png}
    \caption{Compare outputs at different \acrshort{dfc} points with RN50.}
    \label{methods:fig:comparern50}
\end{figure}

To check the functionality, the output and of the quantisation the output of the \acrshort{dfc} is checked at every stage.
The \acrshort{dfc} is able to calculate the output of the compiling network at every step.
With that the postprocessing and the functionality of the quantisation can be checked.
The test is conducted with a test image (see \cref{methods:fig:comparetestpic}).
The Results can be seen in \cref{methods:fig:comparern50}.
First we see that the output from CPU, Nativ and Compiled are the same.
Nativ is the model which results in the translation from onnx to \acrshort{har}.
Compiled is floating point omptimized by the \acrshort{dfc}.
This result confirms that postprocessing is implemented correct otherwise the nativ output would be diffrent from the CPU output.
We see that after quantisation there is a huge shift of the output.
The effect can also be seen on the output of other images.
This indicates that the quantisation is the problem.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Images/Implementation/panorama_00002_0014_2_testIMg.jpg}
    \caption{Testimage [panorama\_00002\_0014\_2] to compare outputs of the \acrshort{dfc}}
    \label{methods:fig:comparetestpic}
\end{figure}

\section{Increase accuracy}

Things mentiond in \cref{section:enhancealgorithm} wouldn't make much sense to applied.
Most of the things mentiond extend the current network with some layers.
Due to the cutting of the network these layers have to be processed on the CPU of the Raspberry Pi which would decrease the throughput of the networks.
In \cref{scetion:methods:contimp} the reason for the bad accuracy is suspected to be the bad quantisation.
The models are all translated to INT8.
In a attemp to increase accuracy as much as possible is converted to INT16.
Other ways to increase the accuracy are better text promts and adjusting the threshold for binary classifications.

\subsection{Adjust threshold}

A way to increase accuracy is to adjust the threshold.
This only works in binary classification.
This can be applied to the classification between indoor and outdoor and indoor construction and architectural.
For the outdoor classes (forest,urban and out construction) nothing like this can be done to increase the accuracy.
This threshold has to be calculate.
For this calculation one has to watch out to use a balanced dataset.
To get a balanced dataset a subset is created so that each class has the same amount of images.
The images are randomly sampled.

This process proved beneficial because it doenst need much computational resources and its easily implemented.


\subsection{Change the cut location
\label{methods:sec:cutlocation}}
A assumption is that the last part of the ResNet's, which is a self attention layer, isnt quantising propely due to some big matrix multipications.
To work against this effect the cut which in any case has to be made can appear earlier in the graph.
This means that only convolutions and additions were quantised and processed on hailo.

\begin{table}[!ht]
    \centering
    \begin{tabular}{l|rrrrr}
        \hline
        Modelname & Accuracy & \makecell{Throughput\\(it/s)} & \makecell{Throughput \\ Image (it/s)} & Throughput std & \makecell{Throughput\\Image std} \\ \hline
        RN50 & 0.795 & 20.30 & 24.00 & 1.44 & 1.19 \\ 
        RN50x4 & 0.800 & 9.30 & 9.75 & 0.33 & 0.33 \\ 
        RN101 & 0.830 & 15.22 & 16.59 & 0.73 & 0.49 \\ 
        TinyClip19M & 0.205 & 28.40 & 36.58 & 2.85 & 3.38 \\ 
        TinyClip30M & 0.617 & 21.61 & 23.84 & 1.35 & 4.85 \\ 
    \end{tabular}
    \caption{Performance table from evalutaion on Hailo 8L where the model graph is cut in a diffrent location.}
    \label{methods:tab:perfHailocut}
\end{table}

In \cref{methods:tab:perfHailocut} we see a decrease in accuracy and throughput.
But the output is grouped that most of the accuracy can be made better by adjusting the threshold.
In %\cref{}
we see the adjusted performance


\subsection{Change quantisation}

The \acrshort{dfc} offers a way to change the quantisation of the weigths.
This is applied via the model script in quantisation.
Due to the limitation that no GPU was available during the project quantising a network to a diffrent type than INT8 took very long.

Due to this reason only TinyCLIP-19M was quantised to INT16.
In the end 66\% of parameters were INT16  but no increase in accuracy was found.

\section{Best Result}

The best result was found to be the model with the RN101 as visual encoder